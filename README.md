# üì£ Advancing Multimodal Emotion Recognition in Comics via Multi-disciplinary, Multi-Task, Multi-Lingual Transfer Learning üì£

## ‚ÑπÔ∏è Summary
ComicEmotion is a research-oriented framework for detecting and analyzing emotions in comics using multimodal generative AI. The project integrates multidisciplinary knowledge (art, linguistics, psychology), multi-task pipelines (emotion classification, intensity, character attribution, scene context), and multilingual support to handle speech balloons, captions, and visual cues.

## ü™ú Goals
- Robust emotion recognition across panels, pages, and entire narratives.
- Jointly model visual, textual, and layout signals.
- Support multiple languages and cross-lingual transfer for low-resource comics.
- Sequential Transfer Learning.

## Key Features
- Multimodal input: images, OCR text, speech-bubble layouts, metadata.
- Multi-task outputs: emotion labels, intensity scores, character-wise emotion attribution, sentiment, sarcasm/irony flags, action-context tags.
- Multilingual NLP: tokenization, translation, and language-specific emotion taxonomies.
- Multidisciplinary priors: visual affect cues, comic-specific conventions, cultural annotations.
- Extensible model zoo: vision encoders, multimodal transformers, graph-based scene models, multimodal LLMs for generative explanations.

## Architecture (high-level)
- Preprocessing: panel extraction, speech-bubble detection, OCR, layout parsing.
- Representation: image features, text embeddings, spatial/layout encodings, character graphs.
- Core models:
    - Vision + Language encoders for classification and retrieval.
    - Graph Neural Networks for character interaction modeling.
    - Multimodal generative models for explanation and counterfactuals.
- Heads:
    - Emotion classification (categorical)
    - Emotion intensity/regression
    - Character attribution / coreference resolution
    - Multilingual normalization & translation
    - Explanation generation (textual)

## Data & Annotation
- Recommended sources: public comic datasets, crowdsourced annotations, simulated augmentations.
- Annotation schema: canonical emotion set (configurable), intensity (0‚Äì1), actor ID, confidence, cultural/context tags, language tag.
- Quality: inter-annotator agreement, adjudication workflow, balanced sampling across languages and art styles.

## Training Strategy
- Pretrain vision and language encoders on broad multimodal corpora.
- Multi-task finetuning with task-specific heads and weighted loss scheduling.
- Curriculum learning: visual cues ‚Üí text cues ‚Üí joint reasoning.
- Data augmentation: style transfer, synthetic speech-bubbles, translation augmentation.

## Evaluation
- Metrics: accuracy/F1 for classification, MAE/RMSE for intensity, precision/recall for attribution, BLEU/ROUGE for generated explanations.
- Robustness tests: cross-style, cross-language, occlusion, noisy-OCR resilience.
- Human evaluation: qualitative assessment for generated explanations and cultural appropriateness.

## Ethics & Responsible Use
- Respect copyright and creator rights for comic content; use licensed or public-domain corpora.
- Document dataset provenance and annotation guidelines.
- Monitor for cultural bias in emotion labels and translations; include mitigation and audit steps.
- Include opt-out and privacy guidance for sensitive content if using real-world material.

## Project Structure (suggested)
- data/ ‚Äî dataset manifests, annotation formats
- src/preproc/ ‚Äî OCR, layout, panel segmentation
- src/models/ ‚Äî model definitions and training scripts
- src/eval/ ‚Äî metrics and evaluation suites
- experiments/ ‚Äî config files, checkpoints, logs
- docs/ ‚Äî annotation guidelines, taxonomy, API docs

## Quick Examples (conceptual)
- Panel-level emotion classification: image + OCR ‚Üí emotion + intensity
- Character-centric timeline: sequence of panels ‚Üí per-character emotion trajectory
- Multilingual normalization: raw text (any language) ‚Üí canonical emotion labels

## Contributing
- Open contribution model: issue templates for data, models, ethics concerns.
- Require reproducible experiments, minimal working example, and dataset citations.
- Follow the annotation and coding style guides in docs/.

## Roadmap
- Expand multilingual taxonomies and low-resource transfer.
- Integrate multimodal LLMs for richer explanations and interactive analysis.
- Benchmark suite across styles, eras, and languages.

## License
- Add project license and dataset-specific licenses; default to an open research-friendly license (choose and document explicitly).

# üì¶ Requirements

We use the following versions of the packages:

```
torch==2.4.0
gradio==4.43.0
pydantic==2.9.0
LLaMA-Factory==0.9.0
transformers==4.44.2
bitsandbytes==0.43.1
```

For fine-tuning, you need to install LLaMA-Factory. Run the following command to install LLaMA-Factory and all the necessary dependencies and updates:

```
bash setup.sh
```

<br>

# üíª Platform and Compute

- For fine-tuning LLMs, we use [**LLaMA-Factory.**](https://github.com/hiyouga/LLaMA-Factory)
- For model checkpoints, we use [**Unsloth.**](https://unsloth.ai/)
- We also use [**Hugging Face.**](https://huggingface.co/)

All experiments have been performed on the High Performance Cluster at [**La Rochelle Universit√©.**](https://www.univ-larochelle.fr/)
