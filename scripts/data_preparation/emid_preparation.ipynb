{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a390373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c81286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0296dbf6ade4063a61f796ae5c4c244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8757953c7f24976bab2b0ee1ca79b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"orrzohar/EMID-Emotion-Matching\", cache_dir=\"/Utilisateurs/umushtaq/emorec_work/mdlt_er/datasets/emid_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bda43ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'image', 'same', 'emotion', 'question', 'answer'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sampling_rate', 'image', 'same', 'emotion', 'question', 'answer'],\n",
       "        num_rows: 6000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d431468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.select_columns(['image', 'same', 'emotion', 'question', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fae2c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375>,\n",
       " 'same': False,\n",
       " 'emotion': '',\n",
       " 'question': 'Do both modalities convey the same mood? Say `yes - emotion` if they do; otherwise say `no`.',\n",
       " 'answer': 'no'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bac62fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6cfdd71b324fdcba8dac50b083e693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6581aeec8c34f62854002c9c57f32dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.filter(lambda example: example['emotion'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f15f0982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'same', 'emotion', 'question', 'answer'],\n",
       "        num_rows: 11962\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'same', 'emotion', 'question', 'answer'],\n",
       "        num_rows: 3038\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5de213b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_selected = ds[\"train\"].select(range(3000)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59604b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'same', 'emotion', 'question', 'answer'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9659698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "237d69c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>same</th>\n",
       "      <th>emotion</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>True</td>\n",
       "      <td>anger</td>\n",
       "      <td>Compare the audio with the image: are they exp...</td>\n",
       "      <td>yes - anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>True</td>\n",
       "      <td>anger</td>\n",
       "      <td>Compare the audio with the image: are they exp...</td>\n",
       "      <td>yes - anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>True</td>\n",
       "      <td>excitement</td>\n",
       "      <td>Judge whether the song and the image share an ...</td>\n",
       "      <td>yes - excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>True</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Do both modalities convey the same mood? Say `...</td>\n",
       "      <td>yes - sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;PIL.JpegImagePlugin.JpegImageFile image mode=...</td>\n",
       "      <td>True</td>\n",
       "      <td>anger</td>\n",
       "      <td>Listen to the audio and inspect the picture. A...</td>\n",
       "      <td>yes - anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  same     emotion  \\\n",
       "0  <PIL.JpegImagePlugin.JpegImageFile image mode=...  True       anger   \n",
       "1  <PIL.JpegImagePlugin.JpegImageFile image mode=...  True       anger   \n",
       "2  <PIL.JpegImagePlugin.JpegImageFile image mode=...  True  excitement   \n",
       "3  <PIL.JpegImagePlugin.JpegImageFile image mode=...  True     sadness   \n",
       "4  <PIL.JpegImagePlugin.JpegImageFile image mode=...  True       anger   \n",
       "\n",
       "                                            question            answer  \n",
       "0  Compare the audio with the image: are they exp...       yes - anger  \n",
       "1  Compare the audio with the image: are they exp...       yes - anger  \n",
       "2  Judge whether the song and the image share an ...  yes - excitement  \n",
       "3  Do both modalities convey the same mood? Say `...     yes - sadness  \n",
       "4  Listen to the audio and inspect the picture. A...       yes - anger  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(ds_selected) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "961b5dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'excitement', 'sadness', 'fear', 'contentment',\n",
       "       'amusement', 'awe'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_instruction(row):\n",
    "    \n",
    "    \n",
    "   #  emotion_classes = ['Calm', 'Excited', 'Contentment', 'Frustrated', 'Sad', 'Aroused',\n",
    "   #     'Alarmed', 'Bored', 'Happy', 'Annoyed', 'Tired', 'Glad']\n",
    "   #  formatted_classes = \", \".join([f'\"{emotion}\"' for emotion in emotion_classes])\n",
    "\n",
    "   instruction = f\"\"\"You are an expert art analyst specializing in emotional interpretation of visual images. Your task is to analyze diverse range of images and identify the single dominant emotion they evoke in a viewer.\n",
    "\n",
    "Analyze the emotional content of this iamge and identify the single most dominant emotion it conveys or evokes.\n",
    "\n",
    "Choose exactly one emotion from the following list:\n",
    "['anger', 'excitement', 'sadness', 'fear', 'contentment', 'amusement', 'awe']\n",
    "\n",
    "Consider the following when making your assessment:\n",
    "- Color palette and tone (warm/cool, saturated/muted)\n",
    "- Composition and visual tension\n",
    "- Subject matter and imagery\n",
    "- Brushwork or line quality (chaotic, gentle, rigid, etc.)\n",
    "- Overall mood the artwork projects\n",
    "\n",
    "Respond only with a JSON object in this exact format:\n",
    "{{\"emotion\": \"<emotion>\"}}\n",
    "\n",
    "Do not include any explanation, commentary, or additional fields.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "   return instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3729395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(row):\n",
    "        \n",
    "    return {\"emotion\": row.emotion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cddcffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"instruction\"] = df.apply(lambda x: generation_instruction(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d0c2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"answer\"] = df.apply(lambda x: build_output(x), axis=1) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5963ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert art analyst specializing in emotional interpretation of visual artwork. Your task is to analyze paintings and drawings and identify the single dominant emotion they evoke in a viewer.\n",
      "\n",
      "Analyze the emotional content of this artwork and identify the single most dominant emotion it conveys or evokes.\n",
      "\n",
      "Choose exactly one emotion from the following list:\n",
      "['Calm', 'Excited', 'Contentment', 'Frustrated', 'Sad', 'Aroused', 'Alarmed', 'Bored', 'Happy', 'Annoyed', 'Tired', 'Glad']\n",
      "\n",
      "Consider the following when making your assessment:\n",
      "- Color palette and tone (warm/cool, saturated/muted)\n",
      "- Composition and visual tension\n",
      "- Subject matter and imagery\n",
      "- Brushwork or line quality (chaotic, gentle, rigid, etc.)\n",
      "- Overall mood the artwork projects\n",
      "\n",
      "Respond only with a JSON object in this exact format:\n",
      "{\"emotion\": \"<emotion>\"}\n",
      "\n",
      "Do not include any explanation, commentary, or additional fields.\n",
      "\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8a77a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emotion': 'anger'}\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_jsonl_dataset(\n",
    "    split,\n",
    "    image_col=\"image\",\n",
    "    instruction_col=\"instruction\",\n",
    "    answer_col=\"answer\",\n",
    "    \n",
    "):  \n",
    "    output_path=f\"emid_{split}.jsonl\"\n",
    "    records = []\n",
    "    \n",
    "    #df = df_train if split == \"train\" else df_test\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sample = {\n",
    "            \"messages\": [\n",
    "            {\n",
    "                \"content\": \"<image>\" + row[instruction_col],\n",
    "                \"role\": \"user\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"\" + str(row[answer_col]),\n",
    "                \"role\": \"assistant\"\n",
    "            },\n",
    "            ],\n",
    "            \"images\": [\n",
    "            row[image_col]\n",
    "            ]\n",
    "        }\n",
    "        records.append(sample)\n",
    "        \n",
    "    output_path = Path(output_path)\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved {len(records)} samples to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_jsonl_dataset(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8011b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "17M_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
